# Practical Lab Course 
The folder contains assignments and course materials
## 1. Assignments

### Project 1: Decision tree

- Utilized the Titanic dataset to perform survival predictions using Decision Trees and Random Forests.
Experiment is completed on Kaggle and exported as an .ipynb file.

### Project 2: Computer Vision

- Assignment1.ipynb (7 points) : 
The notebook introduces a single-layer supervised neural network in Section 1, requires coding and answering specific questions in Sections 2 and 3, and focuses on familiarizing the reader with PyTorch and providing essential debugging strategies for training neural networks in Section 4.

- Assignment2.ipynb (8 points) : The notebook covers data acquisition, offering methods for direct download or Google Colab setup, featuring MSCOCO dataset with 20K training images, 100 validation images, and a JSON file mapping ImageNet labels to categories, with provided code for Colab data loading.

### Project 3: Natural Language Processing

- This assignment focuses on training a model for binary classification of movie reviews (categorized as 'good' or 'bad') using a dataset comprising 40,000 real movie reviews from IMDb.

### Project 4: Large Language Model Fine-Tuning

- An experiment centered on fine-tuning  Llama 2 model, with subsequent evaluation of performance through perplexity and cross-entropy measurements, and suggests improving results by adjusting dataset sizes or tweaking model parameters.
- Upon completion of the assignment produces a csv file, which is submitted to the Kaggle for scoring.

### Project 5: Prompt Engineering and Dialogue Summarization with FLAN-T5

- This notebook focuses on the practical aspect of dialogue summarization of FLAN-T5, emphasizing the exploration of prompt engineering and its impact on model output through comparative analyses of zero-shot, one-shot, and few-shot inferences.


### Project 6: Data-Centric AI - Dataset Curation

- This notebook focuses on analyzing a simulated classification dataset labeled by multiple annotators, exploring exercises to compute majority-vote consensus labels and estimate annotator quality, and employed the CROWDLAB algorithm to estimate consensus labels and annotator quality without access to ground truth labels. 

### Project 7: Data-Centric AI - Label Errors

- Employed confident learning to enhance the accuracy of an XGBoost classifier on a noisy dataset with label errors, optimized the dataset. The process involved establishing a baseline accuracy, identifying and removing mislabeled data points, and retraining the model for improved test accuracy.

### Project 8: Data-Centric AI - Growing Dataset with Active Learning

- This notebook focuses on dataset expansion and active learning strategies, contrasting passive learning (random sampling) with active learning with Max Entropy method. This involves selection of unlabeled examples based on model uncertainty, scaling up with OpenImages dataset and the SEALS approach for efficient active learning and similarity search.

### Project 9: Reinforcement Learning 

- Learning outcomes are summarised with PPT/Slides. The self-engaged learning projects are too large, so it is not included here. 

### Project 10: Recommender System

- The three notebooks for self-learning focuses on item-item recommendation, cold-start problem and implicit feedback recommender. 

- Learning outcomes are summarised with PPT/Slides, with additional self-engaged learning projects in the *project* folder.

### Project 11: Graph Neural Networks

- This experiment explores the practical application of Graph Neural Networks (GNNs) on various tasks, with a focus on basic network layers like graph convolutions and attention layers, and by applying a GNN to node-level, edge-level, and graph-level tasks after importing standard libraries.

- Learning outcomes are summarised with PPT/Slides, with additional self-engaged learning projects in teh *project* folder.


## 2. Course materials (to be updated)
- supplementary notes for starting off lab course
- tutorials on speech and signal processing
