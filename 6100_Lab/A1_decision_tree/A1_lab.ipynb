{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{},"source":["In this notebook, we're going to learn all about Decision Tree, by building one from scratch, and using it to submit to the Titanic competition! That might sound like a pretty big stretch, but I think you'll be surprised to discover how straightforward it actually is.\n","\n","We'll start by importing the basic set of libraries we normally need for data science work, and setting numpy to use our display space more efficiently:"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-14T02:48:11.089711Z","iopub.status.busy":"2023-09-14T02:48:11.089299Z","iopub.status.idle":"2023-09-14T02:48:11.095329Z","shell.execute_reply":"2023-09-14T02:48:11.094262Z","shell.execute_reply.started":"2023-09-14T02:48:11.089678Z"},"trusted":true},"outputs":[],"source":["from fastai.imports import *\n","np.set_printoptions(linewidth=100)"]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["reading data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.098518Z","iopub.status.busy":"2023-09-14T02:48:11.097986Z","iopub.status.idle":"2023-09-14T02:48:11.159469Z","shell.execute_reply":"2023-09-14T02:48:11.158187Z","shell.execute_reply.started":"2023-09-14T02:48:11.098477Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","path = Path('titanic')\n","\n","df = pd.read_csv(path/'train.csv')\n","tst_df = pd.read_csv(path/'test.csv')\n","modes = df.mode().iloc[0]\n","df"]},{"cell_type":"markdown","metadata":{},"source":["We can transform these fields into categorical variables. In Pandas, this conversion essentially enumerates the unique column values, replacing each unique value with a corresponding index."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.162360Z","iopub.status.busy":"2023-09-14T02:48:11.161662Z","iopub.status.idle":"2023-09-14T02:48:11.216583Z","shell.execute_reply":"2023-09-14T02:48:11.215385Z","shell.execute_reply.started":"2023-09-14T02:48:11.162319Z"},"trusted":true},"outputs":[],"source":["def proc_data(df): # Define a function to process the data\n","    df['Fare'] = df.Fare.fillna(0) # Fill any missing values in the 'Fare' column with 0\n","    df.fillna(modes, inplace=True) # Fill any missing values in other columns with pre-calculated modes (commonly occurring values)\n","    \n","    df['LogFare'] = np.log1p(df['Fare']) # Create a new column 'LogFare' which is the logarithm (plus 1) of the 'Fare' column to handle cases where Fare might be 0\n","    df['Embarked'] = pd.Categorical(df.Embarked) # Convert the 'Embarked' column to a categorical datatype, facilitating encoding for modeling\n","    df['Sex'] = pd.Categorical(df.Sex) # Convert the 'Sex' column to a categorical datatype\n","\n","proc_data(df) # Apply the function to the main data (commonly training data)\n","proc_data(tst_df) # Apply the function to the test data\n","df"]},{"cell_type":"markdown","metadata":{},"source":["Let's categorize our data into continuous, categorical, and dependent variables. Notably, Pclass is no longer deemed a categorical variable. Its nature is ordered – 1st, 2nd, and 3rd classes possess a distinct hierarchy. As we'll discover, decision trees primarily focus on this order, rather than the absolute value."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.219200Z","iopub.status.busy":"2023-09-14T02:48:11.218669Z","iopub.status.idle":"2023-09-14T02:48:11.226062Z","shell.execute_reply":"2023-09-14T02:48:11.224506Z","shell.execute_reply.started":"2023-09-14T02:48:11.219155Z"},"trusted":true},"outputs":[],"source":["cats=[\"Sex\",\"Embarked\"]\n","conts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\n","dep=\"Survived\""]},{"cell_type":"markdown","metadata":{},"source":["Even although we've made the `cats` columns categorical, they are still shown by Pandas as their original values:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.230220Z","iopub.status.busy":"2023-09-14T02:48:11.228855Z","iopub.status.idle":"2023-09-14T02:48:11.252172Z","shell.execute_reply":"2023-09-14T02:48:11.250743Z","shell.execute_reply.started":"2023-09-14T02:48:11.230175Z"},"trusted":true},"outputs":[],"source":["df.Sex.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Binary splits"]},{"cell_type":"markdown","metadata":{},"source":["To embark on the Decision Tree journey, we must first grasp the concept of a decision tree, the foundation of numerous models.\n","\n","At the heart of a decision tree lies a binary split. This mechanism sorts rows into one of two groups, contingent on whether they surpass a given column threshold. For instance, dividing our data based on gender – using 0.5 as a threshold for the Sex column (where 0 stands for female and 1 for male) – is a practical illustration. To visualize this, we'll employ the Seaborn library, an enhancement of matplotlib, offering intuitive charts with an appealing design."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.254997Z","iopub.status.busy":"2023-09-14T02:48:11.253922Z","iopub.status.idle":"2023-09-14T02:48:11.860348Z","shell.execute_reply":"2023-09-14T02:48:11.859160Z","shell.execute_reply.started":"2023-09-14T02:48:11.254946Z"},"trusted":true},"outputs":[],"source":["import seaborn as sns\n","\n","fig,axs = plt.subplots(1,2, figsize=(11,5))\n","sns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\n","sns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");"]},{"cell_type":"markdown","metadata":{},"source":["The resulting visualization underscores two salient points: First, when segregated by gender, survival rates vary significantly – over 70% for females and below 20% for males. Second, the distribution is balanced, with each group comprising over 300 of the approximately 900 passengers.\n","\n","Given these insights, we could draft a rudimentary model positing that all females survive, while males don't. Before this, it's prudent to partition our data into training and validation sets, assessing the accuracy of such a premise."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.862941Z","iopub.status.busy":"2023-09-14T02:48:11.862196Z","iopub.status.idle":"2023-09-14T02:48:11.879596Z","shell.execute_reply":"2023-09-14T02:48:11.878004Z","shell.execute_reply.started":"2023-09-14T02:48:11.862895Z"},"trusted":true},"outputs":[],"source":["from numpy import random\n","from sklearn.model_selection import train_test_split\n","\n","random.seed(6100)\n","trn_df,val_df = train_test_split(df, test_size=0.25)\n","trn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\n","val_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)"]},{"cell_type":"markdown","metadata":{},"source":["(In the previous step we also replaced the categorical variables with their integer codes, since some of the models we'll be building in a moment require that.)\n","\n","Now we can create our independent variables (the `x` variables) and dependent (the `y` variable):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.881838Z","iopub.status.busy":"2023-09-14T02:48:11.881391Z","iopub.status.idle":"2023-09-14T02:48:11.898712Z","shell.execute_reply":"2023-09-14T02:48:11.897721Z","shell.execute_reply.started":"2023-09-14T02:48:11.881774Z"},"trusted":true},"outputs":[],"source":["def xs_y(df):\n","    xs = df[cats+conts].copy()\n","    return xs,df[dep] if dep in df else None\n","\n","trn_xs,trn_y = xs_y(trn_df)\n","val_xs,val_y = xs_y(val_df)"]},{"cell_type":"markdown","metadata":{},"source":["Here's the predictions for our extremely simple model, where `female` is coded as `0`:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.901626Z","iopub.status.busy":"2023-09-14T02:48:11.900541Z","iopub.status.idle":"2023-09-14T02:48:11.908655Z","shell.execute_reply":"2023-09-14T02:48:11.907423Z","shell.execute_reply.started":"2023-09-14T02:48:11.901589Z"},"trusted":true},"outputs":[],"source":["preds = val_xs.Sex==0"]},{"cell_type":"markdown","metadata":{},"source":["We'll use mean absolute error to measure how good this model is:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.914483Z","iopub.status.busy":"2023-09-14T02:48:11.914072Z","iopub.status.idle":"2023-09-14T02:48:11.925229Z","shell.execute_reply":"2023-09-14T02:48:11.924335Z","shell.execute_reply.started":"2023-09-14T02:48:11.914449Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","mean_absolute_error(val_y, preds)"]},{"cell_type":"markdown","metadata":{},"source":["Alternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work -- here's an example of how we could look at `LogFare`:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:11.927370Z","iopub.status.busy":"2023-09-14T02:48:11.926440Z","iopub.status.idle":"2023-09-14T02:48:12.475790Z","shell.execute_reply":"2023-09-14T02:48:12.474544Z","shell.execute_reply.started":"2023-09-14T02:48:11.927332Z"},"trusted":true},"outputs":[],"source":["df_fare = trn_df[trn_df.LogFare>0]\n","fig,axs = plt.subplots(1,2, figsize=(11,5))\n","sns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\n","sns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);"]},{"cell_type":"markdown","metadata":{},"source":["The [boxenplot](https://seaborn.pydata.org/generated/seaborn.boxenplot.html) above shows quantiles of `LogFare` for each group of `Survived==0` and `Survived==1`. It shows that the average `LogFare` for passengers that didn't survive is around `2.5`, and for those that did it's around `3.2`. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\n","\n","Let's create a simple model based on this observation:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.479460Z","iopub.status.busy":"2023-09-14T02:48:12.477481Z","iopub.status.idle":"2023-09-14T02:48:12.486562Z","shell.execute_reply":"2023-09-14T02:48:12.485070Z","shell.execute_reply.started":"2023-09-14T02:48:12.479404Z"},"trusted":true},"outputs":[],"source":["preds = val_xs.LogFare>2.6"]},{"cell_type":"markdown","metadata":{},"source":["...and test it out:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.489721Z","iopub.status.busy":"2023-09-14T02:48:12.488782Z","iopub.status.idle":"2023-09-14T02:48:12.506781Z","shell.execute_reply":"2023-09-14T02:48:12.505341Z","shell.execute_reply.started":"2023-09-14T02:48:12.489678Z"},"trusted":true},"outputs":[],"source":["mean_absolute_error(val_y, preds)"]},{"cell_type":"markdown","metadata":{},"source":["This is quite a bit less accurate than our model that used `Sex` as the single binary split.\n","\n","Ideally, we'd like some way to try more columns and breakpoints more easily. We could create a function that returns how good our model is, in order to more quickly try out a few different splits. We'll create a `score` function to do this. Instead of returning the mean absolute error, we'll calculate a measure of *impurity* -- that is, how much the binary split creates two groups where the rows in a group are each similar to each other, or dissimilar.\n","\n","We can measure the similarity of rows inside a group by taking the standard deviation of the dependent variable. If it's higher, then it means the rows are more different to each other. We'll then multiply this by the number of rows, since a bigger group as more impact than a smaller group:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.509473Z","iopub.status.busy":"2023-09-14T02:48:12.508657Z","iopub.status.idle":"2023-09-14T02:48:12.522464Z","shell.execute_reply":"2023-09-14T02:48:12.521479Z","shell.execute_reply.started":"2023-09-14T02:48:12.509423Z"},"trusted":true},"outputs":[],"source":["def _side_score(side, y):\n","    \"\"\"\n","    Calculate the score for a given side (subset) of a binary split.\n","    \n","    :param side: Boolean array indicating which rows are in this subset.\n","    :param y: Target values for the dataset.\n","    :return: Score for this subset, considering its size and target values' variability.\n","    \"\"\"\n","    \n","    # Calculate the total number of True values in the side, representing the size of this subset.\n","    tot = side.sum()\n","    \n","    # If the subset size is less than or equal to 1, return a score of 0.\n","    # This means this subset doesn't provide enough information.\n","    if tot <= 1: \n","        return 0\n","    \n","    # Return the score as the standard deviation of target values in this subset multiplied by its size.\n","    # This considers both the variability in target values and the size of the subset.\n","    return y[side].std() * tot\n"]},{"cell_type":"markdown","metadata":{},"source":["Now we've got that written, we can calculate the score for a split by adding up the scores for the \"left hand side\" (lhs) and \"right hand side\" (rhs):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.524894Z","iopub.status.busy":"2023-09-14T02:48:12.524144Z","iopub.status.idle":"2023-09-14T02:48:12.539286Z","shell.execute_reply":"2023-09-14T02:48:12.537879Z","shell.execute_reply.started":"2023-09-14T02:48:12.524849Z"},"trusted":true},"outputs":[],"source":["def score(col, y, split):\n","    \"\"\"\n","    Calculate the score for a given binary split of a column against a target variable.\n","    \n","    :param col: Column values on which the split is performed.\n","    :param y: Target values for the dataset.\n","    :param split: Threshold value for the binary split.\n","    :return: Score for this binary split, based on its two subsets.\n","    \"\"\"\n","    \n","    # Create a boolean array indicating which rows fall to the left-hand side (lhs) of the split.\n","    # Rows with values less than or equal to the split threshold are on the lhs.\n","    lhs = col <= split\n","    \n","    # Calculate scores for both the left-hand side (lhs) and the right-hand side (~lhs) of the split.\n","    # Then, return the average of the two scores, weighted by the size of the dataset.\n","    return (_side_score(lhs, y) + _side_score(~lhs, y)) / len(y)"]},{"cell_type":"markdown","metadata":{},"source":["For instance, here's the impurity score for the split on `Sex`:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.542040Z","iopub.status.busy":"2023-09-14T02:48:12.540903Z","iopub.status.idle":"2023-09-14T02:48:12.564025Z","shell.execute_reply":"2023-09-14T02:48:12.562831Z","shell.execute_reply.started":"2023-09-14T02:48:12.541980Z"},"trusted":true},"outputs":[],"source":["score(trn_xs[\"Sex\"], trn_y, 0.5)"]},{"cell_type":"markdown","metadata":{},"source":["...and for `LogFare`:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.567212Z","iopub.status.busy":"2023-09-14T02:48:12.566365Z","iopub.status.idle":"2023-09-14T02:48:12.578561Z","shell.execute_reply":"2023-09-14T02:48:12.576917Z","shell.execute_reply.started":"2023-09-14T02:48:12.567158Z"},"trusted":true},"outputs":[],"source":["score(trn_xs[\"LogFare\"], trn_y, 2.6)"]},{"cell_type":"markdown","metadata":{},"source":["As we'd expect from our earlier tests, `Sex` appears to be a better split.\n","\n","To make it easier to find the best binary split, we can create a simple interactive tool (note that this only works in Kaggle):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.581973Z","iopub.status.busy":"2023-09-14T02:48:12.580946Z","iopub.status.idle":"2023-09-14T02:48:12.616662Z","shell.execute_reply":"2023-09-14T02:48:12.615508Z","shell.execute_reply.started":"2023-09-14T02:48:12.581801Z"},"trusted":true},"outputs":[],"source":["def iscore(nm, split):\n","    col = trn_xs[nm]\n","    return score(col, trn_y, split)\n","\n","from ipywidgets import interact\n","interact(nm=conts, split=15.5)(iscore);"]},{"cell_type":"markdown","metadata":{},"source":["Try selecting different columns and split points using the dropdown and slider above. What splits can you find that increase the purity of the data?\n","\n","We can do the same thing for the categorical variables:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.619171Z","iopub.status.busy":"2023-09-14T02:48:12.618770Z","iopub.status.idle":"2023-09-14T02:48:12.653253Z","shell.execute_reply":"2023-09-14T02:48:12.652104Z","shell.execute_reply.started":"2023-09-14T02:48:12.619135Z"},"trusted":true},"outputs":[],"source":["interact(nm=cats, split=2)(iscore);"]},{"cell_type":"markdown","metadata":{},"source":["That works well enough, but it's rather slow and fiddly. Perhaps we could get the computer to automatically find the best split point for a column for us? For example, to find the best split point for `age` we'd first need to make a list of all the possible split points (i.e all the unique values of that field)...:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.655536Z","iopub.status.busy":"2023-09-14T02:48:12.655161Z","iopub.status.idle":"2023-09-14T02:48:12.665038Z","shell.execute_reply":"2023-09-14T02:48:12.663958Z","shell.execute_reply.started":"2023-09-14T02:48:12.655505Z"},"trusted":true},"outputs":[],"source":["nm = \"Age\"\n","col = trn_xs[nm]\n","unq = col.unique()\n","unq.sort()\n","unq"]},{"cell_type":"markdown","metadata":{},"source":["...and find which index of those values is where `score()` is the lowest:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.666570Z","iopub.status.busy":"2023-09-14T02:48:12.666222Z","iopub.status.idle":"2023-09-14T02:48:12.771605Z","shell.execute_reply":"2023-09-14T02:48:12.770069Z","shell.execute_reply.started":"2023-09-14T02:48:12.666538Z"},"trusted":true},"outputs":[],"source":["scores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\n","unq[scores.argmin()]"]},{"cell_type":"markdown","metadata":{},"source":["Based on this, it looks like, for instance, that for the `Age` column, `6` is the optimal cutoff according to our training set.\n","\n","We can write a little function that implements this idea:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.773908Z","iopub.status.busy":"2023-09-14T02:48:12.773381Z","iopub.status.idle":"2023-09-14T02:48:12.876739Z","shell.execute_reply":"2023-09-14T02:48:12.875754Z","shell.execute_reply.started":"2023-09-14T02:48:12.773854Z"},"trusted":true},"outputs":[],"source":["# Define a function that finds the best binary split for a given column.\n","def min_col(df, nm):\n","    # Extract the desired column and the dependent variable from the dataframe.\n","    col, y = df[nm], df[dep]\n","    \n","    # Get unique non-null values from the column.\n","    unq = col.dropna().unique()\n","    \n","    # Calculate the score for each unique value, skipping NaN values.\n","    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n","    \n","    # Find the index of the best (minimum) score.\n","    idx = scores.argmin()\n","    \n","    # Return the unique value associated with the best score and the best score itself.\n","    return unq[idx], scores[idx]\n","\n","# Test the function on the \"Age\" column of the training dataframe.\n","min_col(trn_df, \"Age\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's try all the columns:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:12.879055Z","iopub.status.busy":"2023-09-14T02:48:12.878215Z","iopub.status.idle":"2023-09-14T02:48:13.240724Z","shell.execute_reply":"2023-09-14T02:48:13.239273Z","shell.execute_reply.started":"2023-09-14T02:48:12.879012Z"},"trusted":true},"outputs":[],"source":["cols = cats+conts\n","{o:min_col(trn_df, o) for o in cols}"]},{"cell_type":"markdown","metadata":{},"source":["According to this, `Sex<=0` is the best split we can use.\n","\n","We've just re-invented the [OneR](https://link.springer.com/article/10.1023/A:1022631118932) classifier (or at least, a minor variant of it), which was found to be one of the most effective classifiers in real-world datasets, compared to the algorithms in use in 1993. Since it's so simple and surprisingly effective, it makes for a great *baseline* -- that is, a starting point that you can use to compare your more sophisticated models to.\n","\n","We found earlier that out OneR rule had an error of around `0.215`, so we'll keep that in mind as we try out more sophisticated approaches."]},{"cell_type":"markdown","metadata":{},"source":["## Creating a decision tree"]},{"cell_type":"markdown","metadata":{},"source":["How can we improve our OneR classifier, which predicts survival based only on `Sex`?\n","\n","How about we take each of our two groups, `female` and `male`, and create one more binary split for each of them. That is: fine the single best split for females, and the single best split for males. To do this, all we have to do is repeat the previous section's steps, once for males, and once for females.\n","\n","First, we'll remove `Sex` from the list of possible splits (since we've already used it, and there's only one possible split for that binary column), and create our two groups:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.248787Z","iopub.status.busy":"2023-09-14T02:48:13.248354Z","iopub.status.idle":"2023-09-14T02:48:13.256860Z","shell.execute_reply":"2023-09-14T02:48:13.255675Z","shell.execute_reply.started":"2023-09-14T02:48:13.248749Z"},"trusted":true},"outputs":[],"source":["cols.remove(\"Sex\")\n","ismale = trn_df.Sex==1\n","males,females = trn_df[ismale],trn_df[~ismale]"]},{"cell_type":"markdown","metadata":{},"source":["Now let's find the single best binary split for males...:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.259387Z","iopub.status.busy":"2023-09-14T02:48:13.258633Z","iopub.status.idle":"2023-09-14T02:48:13.523171Z","shell.execute_reply":"2023-09-14T02:48:13.521831Z","shell.execute_reply.started":"2023-09-14T02:48:13.259342Z"},"trusted":true},"outputs":[],"source":["{o:min_col(males, o) for o in cols}"]},{"cell_type":"markdown","metadata":{},"source":["...and for females:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.525396Z","iopub.status.busy":"2023-09-14T02:48:13.524888Z","iopub.status.idle":"2023-09-14T02:48:13.755386Z","shell.execute_reply":"2023-09-14T02:48:13.754376Z","shell.execute_reply.started":"2023-09-14T02:48:13.525362Z"},"trusted":true},"outputs":[],"source":["{o:min_col(females, o) for o in cols}"]},{"cell_type":"markdown","metadata":{},"source":["We can see that the best next binary split for males is `Age<=6`, and for females is `Pclass<=2`.\n","\n","By adding these rules, we have created a *decision tree*, where our model will first check whether `Sex` is female or male, and depending on the result will then check either the above `Age` or `Pclass` rules, as appropriate. We could then repeat the process, creating new additional rules for each of the four groups we've now created.\n","\n","Rather than writing that code manually, we can use `DecisionTreeClassifier`, from *sklearn*, which does exactly that for us:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.757870Z","iopub.status.busy":"2023-09-14T02:48:13.756636Z","iopub.status.idle":"2023-09-14T02:48:13.774247Z","shell.execute_reply":"2023-09-14T02:48:13.773235Z","shell.execute_reply.started":"2023-09-14T02:48:13.757803Z"},"trusted":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier, export_graphviz\n","\n","m = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);"]},{"cell_type":"markdown","metadata":{},"source":["One handy feature or this class is that it provides a function for drawing a tree representing the rules:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.776472Z","iopub.status.busy":"2023-09-14T02:48:13.775530Z","iopub.status.idle":"2023-09-14T02:48:13.790095Z","shell.execute_reply":"2023-09-14T02:48:13.788918Z","shell.execute_reply.started":"2023-09-14T02:48:13.776430Z"},"trusted":true},"outputs":[],"source":["import graphviz\n","\n","def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n","    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n","                      special_characters=True, rotate=False, precision=precision, **kwargs)\n","    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.792078Z","iopub.status.busy":"2023-09-14T02:48:13.791614Z","iopub.status.idle":"2023-09-14T02:48:13.835886Z","shell.execute_reply":"2023-09-14T02:48:13.834447Z","shell.execute_reply.started":"2023-09-14T02:48:13.792034Z"},"trusted":true},"outputs":[],"source":["draw_tree(m, trn_xs, size=10)"]},{"cell_type":"markdown","metadata":{},"source":["We can see that it's found exactly the same splits as we did!\n","\n","In this picture, the more orange nodes have a lower survival rate, and blue have higher survival. Each node shows how many rows (\"*samples*\") match that set of rules, and shows how many perish or survive (\"*values*\"). There's also something called \"*gini*\". That's another measure of impurity, and it's very similar to the `score()` we created earlier. It's defined as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.838983Z","iopub.status.busy":"2023-09-14T02:48:13.837776Z","iopub.status.idle":"2023-09-14T02:48:13.846787Z","shell.execute_reply":"2023-09-14T02:48:13.845533Z","shell.execute_reply.started":"2023-09-14T02:48:13.838932Z"},"trusted":true},"outputs":[],"source":["def gini(cond):\n","    \"\"\"\n","    Compute the Gini impurity for a specified condition.\n","\n","    Parameters:\n","    - cond: A boolean condition for filtering the dataframe.\n","\n","    Returns:\n","    - The Gini impurity value for the filtered data.\n","    \"\"\"\n","    \n","    # Use the given condition to filter the dataframe and select the dependent variable column\n","    act = df.loc[cond, dep]\n","    \n","    # Calculate the Gini impurity:\n","    # Gini impurity is calculated as 1 minus the sum of the squared probabilities of each class.\n","    # In a binary classification (e.g., 0 and 1), if 'act.mean()' gives the probability of class '1', \n","    # then '1 - act.mean()' gives the probability of class '0'.\n","    # The formula then becomes: 1 - probability(1)^2 - probability(0)^2\n","    return 1 - act.mean()**2 - (1-act).mean()**2"]},{"cell_type":"markdown","metadata":{},"source":["What this calculates is the probability that, if you pick two rows from a group, you'll get the same `Survived` result each time. If the group is all the same, the probability is `1.0`, and `0.0` if they're all different:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.849448Z","iopub.status.busy":"2023-09-14T02:48:13.848635Z","iopub.status.idle":"2023-09-14T02:48:13.871960Z","shell.execute_reply":"2023-09-14T02:48:13.870046Z","shell.execute_reply.started":"2023-09-14T02:48:13.849401Z"},"trusted":true},"outputs":[],"source":["gini(df.Sex=='female'), gini(df.Sex=='male')"]},{"cell_type":"markdown","metadata":{},"source":["Let's see how this model compares to our OneR version:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.874976Z","iopub.status.busy":"2023-09-14T02:48:13.873622Z","iopub.status.idle":"2023-09-14T02:48:13.887420Z","shell.execute_reply":"2023-09-14T02:48:13.886232Z","shell.execute_reply.started":"2023-09-14T02:48:13.874924Z"},"trusted":true},"outputs":[],"source":["mean_absolute_error(val_y, m.predict(val_xs))"]},{"cell_type":"markdown","metadata":{},"source":["It's a tiny bit worse. Since this is such a small dataset (we've only got around 200 rows in our validation set) this small difference isn't really meaningful. Perhaps we'll see better results if we create a bigger tree:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.890207Z","iopub.status.busy":"2023-09-14T02:48:13.888998Z","iopub.status.idle":"2023-09-14T02:48:13.937071Z","shell.execute_reply":"2023-09-14T02:48:13.935869Z","shell.execute_reply.started":"2023-09-14T02:48:13.890162Z"},"trusted":true},"outputs":[],"source":["m = DecisionTreeClassifier(min_samples_leaf=50)\n","m.fit(trn_xs, trn_y)\n","draw_tree(m, trn_xs, size=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.939008Z","iopub.status.busy":"2023-09-14T02:48:13.938545Z","iopub.status.idle":"2023-09-14T02:48:13.951982Z","shell.execute_reply":"2023-09-14T02:48:13.950909Z","shell.execute_reply.started":"2023-09-14T02:48:13.938965Z"},"trusted":true},"outputs":[],"source":["mean_absolute_error(val_y, m.predict(val_xs))"]},{"cell_type":"markdown","metadata":{},"source":["It looks like this is an improvement, although again it's a bit hard to tell with small datasets like this. Let's try submitting it to Kaggle:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.954915Z","iopub.status.busy":"2023-09-14T02:48:13.953617Z","iopub.status.idle":"2023-09-14T02:48:13.974741Z","shell.execute_reply":"2023-09-14T02:48:13.973315Z","shell.execute_reply.started":"2023-09-14T02:48:13.954861Z"},"trusted":true},"outputs":[],"source":["tst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes)\n","tst_xs,_ = xs_y(tst_df)\n","\n","def subm(preds, suff):\n","    tst_df['Survived'] = preds\n","    sub_df = tst_df[['PassengerId','Survived']]\n","    sub_df.to_csv(f'submission.csv', index=False)\n","\n","subm(m.predict(tst_xs), 'tree')"]},{"cell_type":"markdown","metadata":{},"source":["When I submitted this, I got a score of 0.763."]},{"cell_type":"markdown","metadata":{},"source":["## The random forest"]},{"cell_type":"markdown","metadata":{},"source":["We can't make the decision tree much bigger than the example above, since some leaf nodes already have only 50 rows in them. That's not a lot of data to make a prediction.\n","\n","So how could we use bigger trees? One big insight came from Leo Breiman: what if we create lots of bigger trees, and take the average of their predictions? Taking the average prediction of a bunch of models in this way is known as [bagging](https://link.springer.com/article/10.1007/BF00058655).\n","\n","The idea is that we want each model's predictions in the averaged ensemble to be uncorrelated with each other model. That way, if we average the predictions, the average will be equal to the true target value -- that's because the average of lots of uncorrelated random errors is zero. That's quite an amazing insight!\n","\n","One way we can create a bunch of uncorrelated models is to train each of them on a different random subset of the data. Here's how we can create a tree on a random subset of the data:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.977357Z","iopub.status.busy":"2023-09-14T02:48:13.976555Z","iopub.status.idle":"2023-09-14T02:48:13.984191Z","shell.execute_reply":"2023-09-14T02:48:13.983150Z","shell.execute_reply.started":"2023-09-14T02:48:13.977318Z"},"trusted":true},"outputs":[],"source":["def get_tree(prop=0.85):\n","    \"\"\"\n","    Generate and fit a decision tree classifier using a subset of the data.\n","\n","    Parameters:\n","    - prop (float, optional): Proportion of the data to use for training. Default is 0.85.\n","\n","    Returns:\n","    - A trained DecisionTreeClassifier.\n","    \"\"\"\n","\n","    # Determine the number of data points in the training set\n","    n = len(trn_y)\n","    \n","    # Randomly select a subset of indices based on the specified proportion\n","    idxs = random.choice(n, int(n*prop))\n","    \n","    # Train a DecisionTreeClassifier on the selected subset and return it\n","    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])"]},{"cell_type":"markdown","metadata":{},"source":["Now we can create as many trees as we want:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:13.986475Z","iopub.status.busy":"2023-09-14T02:48:13.985687Z","iopub.status.idle":"2023-09-14T02:48:14.405248Z","shell.execute_reply":"2023-09-14T02:48:14.404292Z","shell.execute_reply.started":"2023-09-14T02:48:13.986438Z"},"trusted":true},"outputs":[],"source":["trees = [get_tree() for t in range(100)]"]},{"cell_type":"markdown","metadata":{},"source":["Our prediction will be the average of these trees' predictions:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:14.408140Z","iopub.status.busy":"2023-09-14T02:48:14.406917Z","iopub.status.idle":"2023-09-14T02:48:14.571344Z","shell.execute_reply":"2023-09-14T02:48:14.569966Z","shell.execute_reply.started":"2023-09-14T02:48:14.408101Z"},"trusted":true},"outputs":[],"source":["all_probs = [t.predict(val_xs) for t in trees]\n","avg_probs = np.stack(all_probs).mean(0)\n","\n","mean_absolute_error(val_y, avg_probs)"]},{"cell_type":"markdown","metadata":{},"source":["This is nearly identical to what `sklearn`'s `RandomForestClassifier` does. The main extra piece in a \"real\" random forest is that as well as choosing a random sample of data for each tree, it also picks a random subset of columns for each split. Here's how we repeat the above process with a random forest:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T03:07:05.942884Z","iopub.status.busy":"2023-09-14T03:07:05.942338Z","iopub.status.idle":"2023-09-14T03:07:06.423337Z","shell.execute_reply":"2023-09-14T03:07:06.422259Z","shell.execute_reply.started":"2023-09-14T03:07:05.942847Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","rf = RandomForestClassifier(n_estimators=100, min_samples_split=15,random_state=50015720, \n","                             min_samples_leaf=4,oob_score=True, n_jobs = -1)\n","rf.fit(trn_xs, trn_y);\n","mean_absolute_error(val_y, rf.predict(val_xs))\n","\n","y_pred = rf.predict(trn_xs)\n","accuracy= accuracy_score(trn_y ,y_pred)\n","print(f'Accuracy: {accuracy}')\n","print(\"%.4f\" % rf.oob_score_)"]},{"cell_type":"markdown","metadata":{},"source":["We can submit that to Kaggle too:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:15.421135Z","iopub.status.busy":"2023-09-14T02:48:15.420272Z","iopub.status.idle":"2023-09-14T02:48:15.465329Z","shell.execute_reply":"2023-09-14T02:48:15.463993Z","shell.execute_reply.started":"2023-09-14T02:48:15.421102Z"},"trusted":true},"outputs":[],"source":["subm(rf.predict(tst_xs), 'rf')"]},{"cell_type":"markdown","metadata":{},"source":["I found that gave nearly an identical result as our single tree."]},{"cell_type":"markdown","metadata":{},"source":["One particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using `feature_importances_`:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T02:48:15.467618Z","iopub.status.busy":"2023-09-14T02:48:15.467227Z","iopub.status.idle":"2023-09-14T02:48:15.958098Z","shell.execute_reply":"2023-09-14T02:48:15.957067Z","shell.execute_reply.started":"2023-09-14T02:48:15.467586Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');"]},{"cell_type":"markdown","metadata":{},"source":["We can see that `Sex` is by far the most important predictor, with `Pclass` a distant second, and `LogFare` and `Age` behind that. In datasets with many columns, I generally recommend creating a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn't really need to take the `log()` of `Fare`, since random forests only care about order, and `log()` doesn't change the order -- we only did it to make our graphs earlier easier to read.)"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{},"source":["First and foremost, complexity doesn't guarantee superiority in model performance. Our 'OneR' model, with just a single binary split, was nearly on par with the more intricate models. Such simplicity, at times, might be more practical and definitely worth consideration. Interestingly, our random forest did not outshine the lone decision tree.\n","\n","It underscores the importance of benchmarking even the most rudimentary models, checking their adequacy for the task at hand. While it's true that for intricate tasks like recommendation systems, NLP, computer vision, or multivariate time series, simple models may falter, there's no harm in trying them out. After all, with the ease of testing various models, why not explore the simpler ones?\n","\n","Lastly, let's not view random forests as overly intricate. Their essential features were seamlessly implemented in a notebook. Their resilience to challenges like normalization, interactions, or non-linear transformations makes them not only user-friendly but also less prone to errors."]}],"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
